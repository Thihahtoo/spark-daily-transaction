{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/thihahtoo/.local/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/thihahtoo/.local/lib/python3.8/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 11:59:30 WARN Utils: Your hostname, LAPTOP-E01KR4V4 resolves to a loopback address: 127.0.1.1; using 172.19.203.29 instead (on interface eth0)\n",
      "23/03/21 11:59:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 11:59:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('RentSpree').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## read the dataset\n",
    "df=spark.read.option('header','true').csv('daily_transactions/daily_transactions.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- event_date: timestamp (nullable = true)\n",
      " |-- usertype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Check the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------+\n",
      "|                 _id|         event_date|usertype|\n",
      "+--------------------+-------------------+--------+\n",
      "|By1+rEy20nW/sgReh...|2021-02-10 00:00:00|       A|\n",
      "|x7lxSW+y6Kymzxf1y...|2021-02-10 00:00:00|       A|\n",
      "|BQ7feJAnc6ntg4PaO...|2021-02-24 00:00:00|       A|\n",
      "|xmc9CsW3Q7K9HK5+p...|2020-12-08 00:00:00|       A|\n",
      "|Dt0y6YCKnC/mD9Sm1...|2020-06-17 00:00:00|       A|\n",
      "|8VN+wWDZyD+/H7IRI...|2021-01-07 00:00:00|       A|\n",
      "|WXsKNZ5l/rqxIk3pw...|2020-12-18 00:00:00|       A|\n",
      "|vb6FkuOhgdOzmGpgl...|2021-01-18 00:00:00|       A|\n",
      "|hginiEQhCS1lA6bns...|2020-05-02 00:00:00|       A|\n",
      "|TqcP/Vc8nfJIaOG9T...|2020-11-08 00:00:00|       A|\n",
      "+--------------------+-------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Id with more than one usertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id='1pjAQ3L6g2nDMPXdEs+T16fUjZ8=', event_date=datetime.datetime(2020, 4, 21, 0, 0), usertype='D'),\n",
       " Row(_id='z/EFPSvbktIGuiH61IOKnZ3JgTY=', event_date=datetime.datetime(2021, 4, 29, 0, 0), usertype='D'),\n",
       " Row(_id='7Z4YJCwAye7Al3+ZVe7ya5uymPQ=', event_date=datetime.datetime(2020, 7, 29, 0, 0), usertype='D'),\n",
       " Row(_id='ostGCvBSK4ym3oeHL6FEDxUUsOk=', event_date=datetime.datetime(2019, 11, 18, 0, 0), usertype='D'),\n",
       " Row(_id='ostGCvBSK4ym3oeHL6FEDxUUsOk=', event_date=datetime.datetime(2019, 11, 18, 0, 0), usertype='A')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_id='Ughw2F3IDh5eu0XCsZe2VHOHvrk=', event_date=datetime.datetime(2021, 1, 10, 0, 0), usertype='D'),\n",
       " Row(_id='DnlHKHuPAkgzd0sfGuTien15E3o=', event_date=datetime.datetime(2020, 7, 14, 0, 0), usertype='D'),\n",
       " Row(_id='1pjAQ3L6g2nDMPXdEs+T16fUjZ8=', event_date=datetime.datetime(2020, 4, 21, 0, 0), usertype='D'),\n",
       " Row(_id='z/EFPSvbktIGuiH61IOKnZ3JgTY=', event_date=datetime.datetime(2021, 4, 29, 0, 0), usertype='D'),\n",
       " Row(_id='7Z4YJCwAye7Al3+ZVe7ya5uymPQ=', event_date=datetime.datetime(2020, 7, 29, 0, 0), usertype='D')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_df = df.select(['_id','usertype']).distinct().groupBy(['_id']).count()\n",
    "duplicate_id_lst = dis_df.select('_id').filter(dis_df['count']>1).rdd.map(lambda x: x._id).collect()\n",
    "df = df[~df['_id'].isin(duplicate_id_lst)]\n",
    "df.tail(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get new users"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "select month, count(*) from \n",
    "(\n",
    "    select m._id, FORMAT_DATE('%b %Y', m.event_date) as month,m.event_date, first_event, (m.event_date=first_event as new_user \n",
    "    from `thihadataset.daily_transaction` m \n",
    "    inner join \n",
    "    (\n",
    "        SELECT _id, min(event_date) as first_event FROM `thihadataset.daily_transaction` group by 1\n",
    "    ) as f_date \n",
    "    on m._id=f_date._id\n",
    ") \n",
    "where new_user is true\n",
    "group by 1\n",
    "order by 2 desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                       (0 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|         month|new_users|\n",
      "+--------------+---------+\n",
      "|    April 2021|    34927|\n",
      "|    March 2021|    31369|\n",
      "|     July 2020|    30152|\n",
      "|   August 2020|    28019|\n",
      "|  January 2021|    23389|\n",
      "|     June 2020|    22949|\n",
      "|  October 2020|    22854|\n",
      "| February 2021|    22676|\n",
      "|September 2020|    21693|\n",
      "|      May 2020|    20295|\n",
      "| December 2020|    19071|\n",
      "| November 2020|    18875|\n",
      "|    April 2020|    15117|\n",
      "|    March 2020|    13021|\n",
      "|     July 2019|    12985|\n",
      "| February 2020|    12891|\n",
      "|  January 2020|    12732|\n",
      "|     June 2019|    11833|\n",
      "|   August 2019|    11755|\n",
      "|  October 2019|    10318|\n",
      "+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "first_event_df = df.groupBy('_id').agg(F.min('event_date').alias(\"first_event\"))\n",
    "join_df = df.join(first_event_df, df['_id']==first_event_df['_id'], 'inner').drop(first_event_df._id)\n",
    "newuser_df = join_df.withColumn(\"new_user\", join_df[\"event_date\"]==join_df[\"first_event\"]).withColumn('month', F.date_format(join_df[\"event_date\"], \"MMMM yyyy\"))\n",
    "newuser_count_df = newuser_df.filter(newuser_df[\"new_user\"]==True).groupBy(\"month\").count().withColumnRenamed(\"count\",\"new_users\")\n",
    "\n",
    "newuser_count_df.sort(newuser_count_df['new_users'], ascending=False).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get returning users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|usertype|period|\n",
      "+--------+------+\n",
      "|       A|   360|\n",
      "|       B|   360|\n",
      "|       C|   120|\n",
      "|       D|   260|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\",360),\n",
    "        (\"B\",360),\n",
    "        (\"C\",120),\n",
    "        (\"D\",260)]\n",
    "churn_period_df = spark.createDataFrame(data=data,schema=['usertype', 'period'])\n",
    "churn_period_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with churn_period as (   \n",
    "    select 'A' as usertype, 360 as period union all   \n",
    "    select 'B' as usertype, 360 as period union all   \n",
    "    select 'C' as usertype, 120 as period union all   \n",
    "    select 'D' as usertype, 260 as period\n",
    "),\n",
    "date_differ as (  \n",
    "    select _id, event_date, usertype, previous_event, date_diff(event_date, previous_event, DAY) as date_different from   \n",
    "    (   \n",
    "        select *, lag(event_date,1) over (partition by _id order by event_date) as previous_event \n",
    "        from `thihadataset.daily_transaction`        \n",
    "    ) where previous_event is not null\n",
    "),\n",
    "return_user as (  \n",
    "    select _id, event_date, date_differ.usertype, previous_event, date_different, period, (date_different>=period) as returning_user, FORMAT_DATE('%b %Y', event_date) as month  \n",
    "    from date_differ  \n",
    "    inner join churn_period  \n",
    "    on date_differ.usertype=churn_period.usertype\n",
    ")\n",
    "select month, count(*) as returning_users from (select distinct month, _id, returning_user from return_user)\n",
    "where returning_user is true \n",
    "group by 1\n",
    "order by 2 desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|         month|returning_users|\n",
      "+--------------+---------------+\n",
      "|    April 2021|            946|\n",
      "|    March 2021|            719|\n",
      "| February 2021|            569|\n",
      "|  January 2021|            560|\n",
      "|  October 2020|            393|\n",
      "| December 2020|            387|\n",
      "| November 2020|            351|\n",
      "|   August 2020|            347|\n",
      "|     July 2020|            345|\n",
      "|September 2020|            289|\n",
      "|     June 2020|            210|\n",
      "|      May 2020|            157|\n",
      "|      May 2021|             96|\n",
      "|    March 2020|             94|\n",
      "|    April 2020|             86|\n",
      "| February 2020|             81|\n",
      "|  January 2020|             76|\n",
      "| December 2019|             41|\n",
      "| November 2019|             22|\n",
      "|  October 2019|             15|\n",
      "+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"_id\").orderBy(\"event_date\")\n",
    "previous_event_df = df.withColumn(\"previous_event\", F.lag(\"event_date\",1).over(windowSpec)).na.drop()\n",
    "date_diff_df = previous_event_df.withColumn('date_diff', F.datediff(previous_event_df['event_date'], previous_event_df['previous_event']))\n",
    "join_period_df = date_diff_df.join(churn_period_df, date_diff_df['usertype']==churn_period_df['usertype'], 'inner').drop(churn_period_df['usertype'])\n",
    "return_user_df = join_period_df.withColumn(\"return_user\", join_period_df[\"date_diff\"]>=join_period_df[\"period\"]).withColumn('month', F.date_format(df[\"event_date\"], \"MMMM yyyy\"))\n",
    "return_count_df = return_user_df.select(['month', '_id']).distinct().filter(return_user_df[\"return_user\"]==True).groupBy(\"month\").count().withColumnRenamed(\"count\",\"returning_users\")\n",
    "\n",
    "return_count_df.sort(return_count_df[\"returning_users\"], ascending=False).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dropoff users"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with churn_period as (\n",
    "    select 'A' as usertype, 360 as period union all  \n",
    "    select 'B' as usertype, 360 as period union all  \n",
    "    select 'C' as usertype, 120 as period union all  \n",
    "    select 'D' as usertype, 260 as period\n",
    "),\n",
    "date_differ as (  \n",
    "    select _id,event_date,main.usertype,coalesce(next_event, m_date) as next_event,date_diff(coalesce(next_event, m_date), event_date, DAY) as date_different from  \n",
    "    (\n",
    "        select *, lead(event_date,1) over (partition by _id order by event_date) as next_event      \n",
    "        from `thihadataset.daily_transaction` ) main     \n",
    "        cross join   \n",
    "        (select max(event_date) as m_date from `thihadataset.daily_transaction`) max_date\n",
    "),\n",
    "dropped_user as (  \n",
    "    select _id, event_date, main.usertype, next_event, date_different, period,(date_different>=period) as dropped  \n",
    "    from date_differ main  \n",
    "    inner join churn_period  \n",
    "    on main.usertype=churn_period.usertype\n",
    "),\n",
    "dropped_dt as (  \n",
    "    select *, date_add(event_date, INTERVAL period DAY) as dropped_date   \n",
    "    from dropped_user  \n",
    "    where dropped is true\n",
    ")\n",
    "select FORMAT_DATE('%b %Y', dropped_date) as month, count(*) as dropoff_users from (select distinct dropped_date, _id, dropped from dropped_dt \n",
    "group by 1\n",
    "order by 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                       (0 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|         month|dropoff_users|\n",
      "+--------------+-------------+\n",
      "|September 2019|          269|\n",
      "|  October 2019|          516|\n",
      "| December 2019|          534|\n",
      "| November 2019|          550|\n",
      "|    April 2020|          828|\n",
      "|    March 2020|          964|\n",
      "| February 2020|         1025|\n",
      "|  January 2020|         1056|\n",
      "|      May 2021|         2433|\n",
      "|      May 2020|         8111|\n",
      "| December 2020|         9399|\n",
      "| November 2020|         9846|\n",
      "|  October 2020|        10238|\n",
      "|September 2020|        10312|\n",
      "|     June 2020|        10923|\n",
      "|   August 2020|        11222|\n",
      "|    March 2021|        12231|\n",
      "|     July 2020|        12456|\n",
      "| February 2021|        12618|\n",
      "|  January 2021|        13126|\n",
      "+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"_id\").orderBy(\"event_date\")\n",
    "max_date = df.select(F.max('event_date'))\n",
    "next_event_df = df.withColumn(\"next_event\", F.lead(\"event_date\",1).over(windowSpec)).crossJoin(max_date).withColumnRenamed('max(event_date)', 'max_date')\n",
    "next_event_df = next_event_df.select(['_id','event_date','usertype',F.coalesce(next_event_df['next_event'], next_event_df['max_date']).alias('next_event')])\n",
    "date_diff_df = next_event_df.withColumn('date_diff', F.datediff(next_event_df['next_event'], next_event_df['event_date']))\n",
    "join_period_df = date_diff_df.join(churn_period_df, date_diff_df['usertype']==churn_period_df['usertype'], 'inner').drop(churn_period_df['usertype'])\n",
    "dropped_user_df = join_period_df.withColumn(\"dropped\", join_period_df[\"date_diff\"]>=join_period_df[\"period\"]).withColumn(\"period\", join_period_df.period.cast('int'))\n",
    "dropped_dt_df = dropped_user_df.withColumn(\"dropped_date\", F.date_add(dropped_user_df['event_date'], dropped_user_df['period'])).filter(dropped_user_df[\"dropped\"]==True)\n",
    "dropped_mn_df = dropped_dt_df.withColumn('month', F.date_format(dropped_dt_df[\"dropped_date\"], \"MMMM yyyy\"))\n",
    "dropped_count_df = dropped_mn_df.select(['month', '_id', 'dropped']).distinct().groupBy(\"month\").count().withColumnRenamed(\"count\",\"dropoff_users\")\n",
    "\n",
    "dropped_count_df.sort(dropped_count_df['dropoff_users']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 11:59:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/21 11:59:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+---------------+---------+\n",
      "|        month|dropoff_users|returning_users|new_users|\n",
      "+-------------+-------------+---------------+---------+\n",
      "|   April 2020|          828|             86|    15117|\n",
      "|   April 2021|        15101|            946|    34927|\n",
      "|  August 2019|            0|              0|    11755|\n",
      "|  August 2020|        11222|            347|    28019|\n",
      "|December 2019|          534|             41|     8567|\n",
      "|December 2020|         9399|            387|    19071|\n",
      "|February 2020|         1025|             81|    12891|\n",
      "|February 2021|        12618|            569|    22676|\n",
      "| January 2020|         1056|             76|    12732|\n",
      "| January 2021|        13126|            560|    23389|\n",
      "|    July 2019|            0|              0|    12985|\n",
      "|    July 2020|        12456|            345|    30152|\n",
      "|    June 2019|            0|              0|    11833|\n",
      "|    June 2020|        10923|            210|    22949|\n",
      "|   March 2020|          964|             94|    13021|\n",
      "|   March 2021|        12231|            719|    31369|\n",
      "|     May 2019|            0|              0|     7856|\n",
      "|     May 2020|         8111|            157|    20295|\n",
      "|     May 2021|         2433|             96|     3314|\n",
      "|November 2019|          550|             22|     8884|\n",
      "+-------------+-------------+---------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df = dropped_count_df.join(return_count_df, dropped_count_df['month']==return_count_df['month'], 'outer').withColumn('c_month', F.coalesce(dropped_count_df['month'], return_count_df['month'])).drop('month')\n",
    "result_df = result_df.join(newuser_count_df, result_df['c_month']==newuser_count_df['month'], 'outer').withColumn('c_month', F.coalesce(result_df['c_month'], newuser_count_df['month'])).drop('month')\n",
    "result_df = result_df.withColumnRenamed('c_month', 'month').na.fill(0).select(['month', 'dropoff_users', 'returning_users', 'new_users'])\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 140:===================>                                     (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 12:00:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/21 12:00:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+---------------+---------+\n",
      "|         month|dropoff_users|returning_users|new_users|\n",
      "+--------------+-------------+---------------+---------+\n",
      "|    April 2020|          828|             86|    15117|\n",
      "|      May 2020|         8111|            157|    20295|\n",
      "|     June 2020|        10923|            210|    22949|\n",
      "|     July 2020|        12456|            345|    30152|\n",
      "|   August 2020|        11222|            347|    28019|\n",
      "|September 2020|        10312|            289|    21693|\n",
      "|  October 2020|        10238|            393|    22854|\n",
      "| November 2020|         9846|            351|    18875|\n",
      "| December 2020|         9399|            387|    19071|\n",
      "|  January 2021|        13126|            560|    23389|\n",
      "| February 2021|        12618|            569|    22676|\n",
      "|    March 2021|        12231|            719|    31369|\n",
      "|    April 2021|        15101|            946|    34927|\n",
      "+--------------+-------------+---------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from_date = 'April 2020'\n",
    "to_date = 'April 2021'\n",
    "\n",
    "from_date = datetime.strptime(from_date, \"%B %Y\").date()\n",
    "to_date = datetime.strptime(to_date, \"%B %Y\").date()\n",
    "\n",
    "result_df = result_df.withColumn('month_num',F.to_date(result_df['month'],\"MMMM yyyy\"))\n",
    "result_df = result_df.filter(F.col('month_num').between(from_date, to_date)).sort(result_df['month_num']).drop('month_num')\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:===================>                                     (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 12:00:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/21 12:00:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "result_df.coalesce(1).write.csv(path=\"result\", header=True, mode=\"overwrite\")\n",
    "\n",
    "file_name=list(filter(lambda x: x.endswith('.csv'), shutil.os.listdir('result')))[0]\n",
    "shutil.move(f'result/{file_name}', 'result.csv')\n",
    "shutil.rmtree('result', ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
